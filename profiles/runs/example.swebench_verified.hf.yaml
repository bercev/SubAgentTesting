benchmark:
  name: swebench_verified
  dataset_name: SWE-bench/SWE-bench_Verified
  split: test
  data_source: hf
  data_root:
  params:
    # Optional benchmark-specific passthrough values consumed by adapter/evaluator.
    # Example: subset: mini
    {}

evaluation:
  harness_cmd: python -m swebench.harness.run_evaluation
  eval_root: ./external/SWE-bench
  workdir: .
  params:
    # Optional evaluator-specific passthrough values.
    # Example: timeout_s: 900
    {}

runtime:
  mode: patch_only
  selector: 10
  max_tool_calls: 20
  max_wall_time_s: 600

output:
  artifacts_dir: artifacts
